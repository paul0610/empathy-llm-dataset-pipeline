#!/usr/bin/env python3
"""
ConversiÃ³n del modelo de empatÃ­a fusionado a formato GGUF F16
Optimizado para llama.cpp y posterior cuantizaciÃ³n a Q4_K_M
VERSIÃ“N F16: Para mÃ¡xima calidad en cuantizaciÃ³n posterior
"""

import subprocess
import os
import glob
import json
from datetime import datetime

def find_latest_fused_model():
    """Encontrar el modelo fusionado mÃ¡s reciente"""
    
    model_dirs = glob.glob("./empathy-fused-model_*")
    
    if not model_dirs:
        raise FileNotFoundError("No se encontrÃ³ modelo fusionado. Ejecuta 3_fusion_empathy_model.py primero.")
    
    # Ordenar por timestamp y tomar el mÃ¡s reciente
    latest_model = sorted(model_dirs)[-1]
    print(f"ğŸ“ Modelo fusionado mÃ¡s reciente: {latest_model}")
    
    return latest_model

def load_fusion_metadata(model_path):
    """Cargar metadatos de la fusiÃ³n"""
    
    metadata_file = os.path.join(model_path, "fusion_metadata.json")
    
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        return metadata
    else:
        return None

def setup_llama_cpp_path():
    """Configurar ruta de llama.cpp - ajustar segÃºn tu instalaciÃ³n"""
    
    # Rutas comunes donde podrÃ­a estar llama.cpp
    possible_paths = [
        "./llama.cpp",
        "../llama.cpp",
        "~/llama.cpp",
        "/usr/local/llama.cpp",
        "/workspace/llama.cpp"
    ]
    
    for path in possible_paths:
        expanded_path = os.path.expanduser(path)
        convert_script = os.path.join(expanded_path, "convert_hf_to_gguf.py")
        
        if os.path.exists(convert_script):
            print(f"âœ… llama.cpp encontrado en: {expanded_path}")
            return expanded_path
    
    # Si no se encuentra, pedir al usuario
    print("âš ï¸ No se encontrÃ³ llama.cpp automÃ¡ticamente.")
    print("ğŸ’¡ Por favor, especifica la ruta donde tienes clonado llama.cpp:")
    print("   Ejemplo: /workspace/llama.cpp")
    
    user_path = input("ğŸ“ Ruta de llama.cpp: ").strip()
    
    if os.path.exists(os.path.join(user_path, "convert_hf_to_gguf.py")):
        return user_path
    else:
        raise FileNotFoundError(f"No se encontrÃ³ convert_hf_to_gguf.py en {user_path}")

def convert_to_gguf_f16(model_dir, llama_cpp_path):
    """Convertir modelo fusionado a F16 GGUF (mÃ¡xima calidad)"""
    
    print("ğŸ”„ Convirtiendo modelo de empatÃ­a a F16 GGUF...")
    print(f"ğŸ“‚ Modelo fuente: {model_dir}")
    print("ğŸ’ Formato F16: MÃ¡xima calidad para cuantizaciÃ³n posterior")
    
    # Crear directorio de salida
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    gguf_dir = f"./empathy-gguf-f16_{timestamp}"
    os.makedirs(gguf_dir, exist_ok=True)
    
    # Archivo de salida F16
    output_file = f"{gguf_dir}/empathy-llama-f16.gguf"
    
    # Script de conversiÃ³n
    convert_script = os.path.join(llama_cpp_path, "convert_hf_to_gguf.py")
    
    # Comando de conversiÃ³n a F16
    cmd = [
        "python", convert_script,
        model_dir,                    # Directorio del modelo fusionado
        "--outfile", output_file,     # Archivo de salida especÃ­fico
        "--outtype", "f16"           # ConversiÃ³n a F16 (16-bit float, mÃ¡xima calidad)
    ]
    
    print(f"ğŸš€ Ejecutando conversiÃ³n: {' '.join(cmd)}")
    print("ğŸ“‹ MÃ©todo: ConversiÃ³n directa HF â†’ F16 (preserva calidad completa)")
    
    try:
        # Ejecutar conversiÃ³n
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
        
        print("\nğŸ“‹ Salida del proceso de conversiÃ³n:")
        if result.stdout:
            print(result.stdout)
        
        if result.stderr:
            print("\nâš ï¸ Mensajes de advertencia:")
            print(result.stderr)
        
        if result.returncode == 0:
            print("\nâœ… ConversiÃ³n a F16 exitosa!")
            
            # Verificar archivo creado
            if os.path.exists(output_file):
                size_mb = os.path.getsize(output_file) / (1024*1024)
                print(f"ğŸ“¦ Archivo creado: empathy-llama-f16.gguf")
                print(f"ğŸ“Š TamaÃ±o: {size_mb:.1f} MB")
                print(f"ğŸ¯ TamaÃ±o esperado para F16: ~2400-2600 MB")
                
                # Crear metadatos del GGUF
                gguf_metadata = {
                    "source_model": model_dir,
                    "output_file": output_file,
                    "format": "GGUF F16",
                    "size_mb": round(size_mb, 1),
                    "conversion_timestamp": datetime.now().isoformat(),
                    "capabilities": [
                        "empathy_detection",
                        "crisis_detection",
                        "multimodal_text_analysis",
                        "spanish_peru_localization",
                        "mobile_optimized"
                    ],
                    "target_platforms": [
                        "llama.cpp",
                        "Android (via llama.cpp)",
                        "iOS (via llama.cpp)",
                        "React Native"
                    ],
                    "quantization": {
                        "type": "F16",
                        "bits_per_weight": 16,
                        "quality": "Maximum (100% quality retention)",
                        "speed": "Baseline",
                        "memory_usage": "High",
                        "notes": "Base format for optimal quantization to Q4_K_M, Q5_K_M, Q8_0, etc."
                    }
                }
                
                metadata_file = f"{gguf_dir}/gguf_metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(gguf_metadata, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ“„ Metadatos guardados en: {metadata_file}")
                
                return output_file, gguf_dir
                
            else:
                print("âŒ Archivo GGUF no encontrado despuÃ©s de la conversiÃ³n")
                return None, None
                
        else:
            print(f"âŒ Error en conversiÃ³n (cÃ³digo de salida: {result.returncode})")
            return None, None
            
    except Exception as e:
        print(f"âŒ Error durante la conversiÃ³n: {e}")
        return None, None

def main():
    """FunciÃ³n principal de conversiÃ³n"""
    
    print("ğŸ¯ ConversiÃ³n del Modelo de EmpatÃ­a a GGUF F16")
    print("=" * 60)
    print(f"ğŸ• Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ’ Formato F16: MÃ¡xima calidad para cuantizaciÃ³n posterior")
    
    try:
        # 1. Encontrar modelo fusionado mÃ¡s reciente
        model_dir = find_latest_fused_model()
        
        # 2. Cargar metadatos si existen
        metadata = load_fusion_metadata(model_dir)
        if metadata:
            print(f"ğŸ§  Capacidades: {', '.join(metadata.get('capabilities', []))}")
            print(f"âš¡ TÃ©cnica: {metadata.get('technique', 'DoRA')}")
        
        # 3. Configurar llama.cpp
        print(f"\nğŸ”§ Configurando llama.cpp...")
        llama_cpp_path = setup_llama_cpp_path()
        
        # 4. ConversiÃ³n a F16
        print(f"\nğŸš€ Iniciando conversiÃ³n a GGUF F16...")
        output_file, gguf_dir = convert_to_gguf_f16(model_dir, llama_cpp_path)
        
        if output_file and os.path.exists(output_file):
            print(f"\nğŸ‰ Â¡ConversiÃ³n completada exitosamente!")
            print("=" * 60)
            print(f"ğŸ“ Archivo GGUF: {output_file}")
            print(f"ğŸ“‚ Directorio: {gguf_dir}")
            print(f"ğŸ¯ Formato: F16 (16-bit float, mÃ¡xima calidad)")
            print(f"ğŸ“± Optimizado para: CuantizaciÃ³n posterior")
            print(f"ğŸŒ Idioma: EspaÃ±ol (PerÃº)")
            print(f"ğŸ§  Capacidades: EmpatÃ­a + Crisis + Multimodal")
            print(f"\nğŸ“‹ PrÃ³ximo paso: Ejecutar 5_quantize_empathy_q4.py para Q4_K_M")
            print(f"ğŸ’¡ Nota: El script 5 detectarÃ¡ automÃ¡ticamente el archivo F16")
            
        else:
            print(f"\nâŒ ConversiÃ³n F16 fallÃ³")
        
    except Exception as e:
        print(f"\nâŒ Error durante la conversiÃ³n: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

