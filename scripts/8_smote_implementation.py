#!/usr/bin/env python3
"""
Implementaci√≥n SMOTE para Dataset de Empat√≠a
Balanceamiento de clases usando Synthetic Minority Oversampling Technique
"""

import json
import numpy as np
import random
from typing import List, Dict, Tuple
from collections import Counter
import math

class SimpleSMOTE:
    """
    Implementaci√≥n simplificada de SMOTE para balancear dataset de di√°logos
    Basada en el paper original de Chawla et al. (2002)
    """
    
    def __init__(self, k_neighbors: int = 5, random_state: int = 42):
        self.k_neighbors = k_neighbors
        self.random_state = random_state
        random.seed(random_state)
        np.random.seed(random_state)
    
    def euclidean_distance(self, x1: List[float], x2: List[float]) -> float:
        """Calcular distancia euclidiana entre dos vectores"""
        return math.sqrt(sum((a - b) ** 2 for a, b in zip(x1, x2)))
    
    def find_k_neighbors(self, sample: List[float], samples: List[List[float]], k: int) -> List[int]:
        """Encontrar k vecinos m√°s cercanos usando distancia euclidiana"""
        distances = []
        
        for i, other_sample in enumerate(samples):
            # Comparar por √≠ndice en lugar de contenido para evitar problemas de precisi√≥n
            dist = self.euclidean_distance(sample, other_sample)
            distances.append((dist, i))
        
        # Ordenar por distancia y tomar los k m√°s cercanos (excluyendo distancia 0 si existe)
        distances.sort(key=lambda x: x[0])
        
        # Filtrar distancia 0 (mismo elemento) y tomar k vecinos
        neighbors = []
        for dist, idx in distances:
            if dist > 0 and len(neighbors) < k:  # Excluir distancia 0
                neighbors.append(idx)
        
        # Si no hay suficientes vecinos, usar todos los disponibles
        if len(neighbors) == 0 and len(distances) > 0:
            # Tomar el m√°s cercano aunque sea distancia 0
            neighbors = [distances[0][1]]
        
        return neighbors
    
    def generate_synthetic_sample(self, sample: List[float], neighbor: List[float]) -> List[float]:
        """Generar muestra sint√©tica entre sample y neighbor"""
        # Factor aleatorio entre 0 y 1
        gap = random.random()
        
        # Interpolaci√≥n lineal: sample + gap * (neighbor - sample)
        synthetic = []
        for i in range(len(sample)):
            synthetic_value = sample[i] + gap * (neighbor[i] - sample[i])
            synthetic.append(synthetic_value)
        
        return synthetic
    
    def fit_resample(self, X: List[List[float]], y: List[str], target_distribution: Dict[str, int]) -> Tuple[List[List[float]], List[str]]:
        """
        Aplicar SMOTE para balancear las clases
        
        Args:
            X: Lista de vectores de caracter√≠sticas (embeddings)
            y: Lista de etiquetas de clase
            target_distribution: Distribuci√≥n objetivo {clase: cantidad}
        
        Returns:
            X_resampled, y_resampled: Datos balanceados
        """
        print("üîÑ Aplicando SMOTE para balancear clases...")
        
        # Contar clases actuales
        current_counts = Counter(y)
        print(f"üìä Distribuci√≥n actual: {dict(current_counts)}")
        print(f"üéØ Distribuci√≥n objetivo: {target_distribution}")
        
        # Organizar datos por clase
        class_data = {}
        for i, label in enumerate(y):
            if label not in class_data:
                class_data[label] = []
            class_data[label].append((X[i], i))
        
        # Datos resultantes
        X_resampled = X.copy()
        y_resampled = y.copy()
        
        # Aplicar SMOTE a cada clase minoritaria
        for class_label, target_count in target_distribution.items():
            current_count = current_counts.get(class_label, 0)
            
            if target_count > current_count:
                samples_needed = target_count - current_count
                print(f"üîß Generando {samples_needed} muestras sint√©ticas para clase '{class_label}'")
                
                # Obtener muestras de la clase
                class_samples = [sample for sample, _ in class_data[class_label]]
                
                if len(class_samples) < 2:
                    print(f"‚ö†Ô∏è Clase '{class_label}' tiene muy pocas muestras para SMOTE")
                    continue
                
                # Generar muestras sint√©ticas
                for _ in range(samples_needed):
                    # Seleccionar muestra aleatoria de la clase
                    sample_idx = random.randint(0, len(class_samples) - 1)
                    sample = class_samples[sample_idx]
                    
                    # Encontrar vecinos m√°s cercanos
                    k = min(self.k_neighbors, len(class_samples) - 1)
                    if k <= 0:
                        k = 1
                    
                    neighbor_indices = self.find_k_neighbors(sample, class_samples, k)
                    
                    # Seleccionar vecino aleatorio
                    if neighbor_indices:
                        neighbor_idx = random.choice(neighbor_indices)
                        neighbor = class_samples[neighbor_idx]
                    else:
                        # Fallback: usar una muestra aleatoria diferente
                        available_indices = [i for i in range(len(class_samples)) if class_samples[i] != sample]
                        if available_indices:
                            neighbor_idx = random.choice(available_indices)
                            neighbor = class_samples[neighbor_idx]
                        else:
                            # √öltimo recurso: usar la misma muestra con ruido
                            neighbor = [x + random.uniform(-0.1, 0.1) for x in sample]
                    
                    # Generar muestra sint√©tica
                    synthetic_sample = self.generate_synthetic_sample(sample, neighbor)
                    
                    # Agregar a los datos
                    X_resampled.append(synthetic_sample)
                    y_resampled.append(class_label)
        
        # Verificar resultado
        final_counts = Counter(y_resampled)
        print(f"‚úÖ Distribuci√≥n final: {dict(final_counts)}")
        
        return X_resampled, y_resampled

class DialogueEmbedder:
    """
    Generador de embeddings simplificado para di√°logos
    Convierte texto a vectores num√©ricos para aplicar SMOTE
    """
    
    def __init__(self):
        # Vocabulario de caracter√≠sticas emocionales
        self.emotional_features = {
            # Palabras de crisis
            'crisis_words': ['suicidio', 'muerte', 'morir', 'acabar', 'terminar', 'basta', 'no puedo', 'imposible'],
            'depression_words': ['triste', 'deprimido', 'vac√≠o', 'sin ganas', 'cansado', 'agotado', 'desesperanza'],
            'anxiety_words': ['nervioso', 'ansioso', 'miedo', 'p√°nico', 'preocupado', 'estresado', 'agobiado'],
            'anger_words': ['enojado', 'furioso', 'molesto', 'irritado', 'odio', 'rabia', 'ira'],
            'positive_words': ['feliz', 'contento', 'bien', 'mejor', 'esperanza', 'optimista', 'tranquilo'],
            
            # Patrones de escritura
            'exclamations': ['!', '!!', '!!!'],
            'questions': ['?', '??', '???'],
            'ellipsis': ['...', '....', '.....'],
            'caps': ['MAY√öSCULAS'],
            
            # Emojis por categor√≠a
            'sad_emojis': ['üòî', 'üò¢', 'üò≠', 'üíî', 'üòû'],
            'happy_emojis': ['üòä', 'üòÑ', 'üòÅ', 'ü•∞', 'üòç'],
            'crisis_emojis': ['üíÄ', '‚ö∞Ô∏è', 'üî™', 'üíä'],
            'anxiety_emojis': ['üò∞', 'üò®', 'üò±', 'ü§Ø']
        }
        
        # Crear vocabulario completo
        self.vocab = {}
        idx = 0
        for category, words in self.emotional_features.items():
            for word in words:
                self.vocab[word] = idx
                idx += 1
        
        self.vocab_size = len(self.vocab)
        print(f"üìù Vocabulario emocional creado: {self.vocab_size} caracter√≠sticas")
    
    def text_to_embedding(self, text: str) -> List[float]:
        """Convertir texto a vector de caracter√≠sticas emocionales"""
        text_lower = text.lower()
        embedding = [0.0] * self.vocab_size
        
        # Contar caracter√≠sticas
        for word, idx in self.vocab.items():
            if word in text_lower:
                # Contar frecuencia normalizada
                count = text_lower.count(word)
                embedding[idx] = count / len(text.split())
        
        # Caracter√≠sticas adicionales
        additional_features = [
            len(text.split()),  # Longitud en palabras
            text.count('!') / len(text),  # Densidad de exclamaciones
            text.count('?') / len(text),  # Densidad de preguntas
            text.count('.') / len(text),  # Densidad de puntos
            sum(1 for c in text if c.isupper()) / len(text),  # Proporci√≥n may√∫sculas
            len([c for c in text if ord(c) > 127]) / len(text)  # Proporci√≥n emojis/caracteres especiales
        ]
        
        embedding.extend(additional_features)
        return embedding
    
    def dialogue_to_embedding(self, dialogue: Dict) -> List[float]:
        """Convertir di√°logo completo a embedding"""
        user_text = dialogue['turns'][0]['text']
        assistant_text = dialogue['turns'][1]['text']
        
        # Embeddings separados
        user_emb = self.text_to_embedding(user_text)
        assistant_emb = self.text_to_embedding(assistant_text)
        
        # Combinar embeddings (concatenaci√≥n + estad√≠sticas)
        combined = user_emb + assistant_emb
        
        # Estad√≠sticas adicionales
        combined.extend([
            np.mean(user_emb),  # Media del embedding del usuario
            np.std(user_emb),   # Desviaci√≥n est√°ndar
            np.mean(assistant_emb),
            np.std(assistant_emb)
        ])
        
        return combined

class SMOTEDatasetBalancer:
    """
    Balanceador de dataset usando SMOTE para di√°logos emp√°ticos
    """
    
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        self.embedder = DialogueEmbedder()
        self.smote = SimpleSMOTE(k_neighbors=5, random_state=42)
        
    def load_dataset(self) -> List[Dict]:
        """Cargar dataset desde archivo JSONL"""
        examples = []
        
        with open(self.input_file, 'r', encoding='utf-8') as f:
            for line in f:
                examples.append(json.loads(line.strip()))
        
        print(f"üìÅ Dataset cargado: {len(examples)} ejemplos")
        return examples
    
    def analyze_class_distribution(self, examples: List[Dict]) -> Dict[str, int]:
        """Analizar distribuci√≥n actual de clases"""
        risk_classes = [ex['labels']['risk_class'] for ex in examples]
        distribution = Counter(risk_classes)
        
        print("\nüìä DISTRIBUCI√ìN ACTUAL DE CLASES:")
        print("=" * 50)
        total = len(examples)
        
        for risk_class, count in sorted(distribution.items()):
            percentage = (count / total) * 100
            print(f"{risk_class:20s}: {count:5d} ({percentage:5.1f}%)")
        
        return dict(distribution)
    
    def define_target_distribution(self, current_dist: Dict[str, int]) -> Dict[str, int]:
        """Definir distribuci√≥n objetivo balanceada"""
        # Estrategia: Balancear clases minoritarias manteniendo proporci√≥n realista
        
        target_dist = {
            'LOW_DISTRESS': current_dist.get('LOW_DISTRESS', 0),  # Mantener clase mayoritaria
            'NO_CRISIS': current_dist.get('NO_CRISIS', 0),       # Mantener
            'MODERATE': max(2000, current_dist.get('MODERATE', 0)),  # Aumentar a 2000
            'HIGH_SUICIDE_RISK': max(2000, current_dist.get('HIGH_SUICIDE_RISK', 0)),  # Cr√≠tico: aumentar
            'SELF_HARM_RISK': max(1500, current_dist.get('SELF_HARM_RISK', 0)),
            'VIOLENCE_RISK': max(1500, current_dist.get('VIOLENCE_RISK', 0))
        }
        
        print("\nüéØ DISTRIBUCI√ìN OBJETIVO:")
        print("=" * 50)
        total_target = sum(target_dist.values())
        
        for risk_class, count in sorted(target_dist.items()):
            current = current_dist.get(risk_class, 0)
            increase = count - current
            percentage = (count / total_target) * 100
            
            if increase > 0:
                print(f"{risk_class:20s}: {count:5d} ({percentage:5.1f}%) [+{increase}]")
            else:
                print(f"{risk_class:20s}: {count:5d} ({percentage:5.1f}%)")
        
        return target_dist
    
    def convert_to_embeddings(self, examples: List[Dict]) -> Tuple[List[List[float]], List[str], List[Dict]]:
        """Convertir di√°logos a embeddings para SMOTE"""
        print("\nüîÑ Convirtiendo di√°logos a embeddings...")
        
        embeddings = []
        labels = []
        
        for i, example in enumerate(examples):
            embedding = self.embedder.dialogue_to_embedding(example)
            embeddings.append(embedding)
            labels.append(example['labels']['risk_class'])
            
            if (i + 1) % 1000 == 0:
                print(f"  ‚úÖ {i + 1}/{len(examples)} embeddings generados")
        
        print(f"‚úÖ Embeddings completados: {len(embeddings)} vectores de {len(embeddings[0])} dimensiones")
        return embeddings, labels, examples
    
    def embedding_to_dialogue(self, embedding: List[float], reference_examples: List[Dict], target_class: str) -> Dict:
        """Convertir embedding sint√©tico de vuelta a di√°logo"""
        # Encontrar el ejemplo m√°s similar de la clase objetivo
        class_examples = [ex for ex in reference_examples if ex['labels']['risk_class'] == target_class]
        
        if not class_examples:
            # Fallback: usar cualquier ejemplo de la clase
            class_examples = reference_examples
        
        # Seleccionar ejemplo base aleatorio
        base_example = random.choice(class_examples)
        
        # Crear nuevo di√°logo basado en el ejemplo base pero con variaciones
        synthetic_dialogue = {
            "dialog_id": f"tfm-smote-{target_class.lower()}-{random.randint(1000, 9999)}",
            "turns": base_example["turns"].copy(),  # Usar como base
            "labels": {
                "risk_class": target_class,
                "risk_signals": base_example["labels"]["risk_signals"].copy(),
                "techniques": base_example["labels"]["techniques"].copy(),
                "needs_rag": target_class in ["HIGH_SUICIDE_RISK", "SELF_HARM_RISK", "VIOLENCE_RISK", "MODERATE"]
            },
            "meta": {
                "language": "es-PE",
                "domain": ["smote_synthetic"],
                "style": ["empathetic"],
                "generation_method": "SMOTE_interpolation"
            }
        }
        
        return synthetic_dialogue
    
    def apply_smote_balancing(self):
        """Aplicar SMOTE completo al dataset"""
        print("üéØ Iniciando Balanceamiento SMOTE del Dataset")
        print("=" * 60)
        
        # 1. Cargar dataset
        examples = self.load_dataset()
        
        # 2. Analizar distribuci√≥n actual
        current_dist = self.analyze_class_distribution(examples)
        
        # 3. Definir distribuci√≥n objetivo
        target_dist = self.define_target_distribution(current_dist)
        
        # 4. Convertir a embeddings
        embeddings, labels, original_examples = self.convert_to_embeddings(examples)
        
        # 5. Aplicar SMOTE
        print("\nüîß Aplicando SMOTE...")
        balanced_embeddings, balanced_labels = self.smote.fit_resample(embeddings, labels, target_dist)
        
        # 6. Convertir embeddings sint√©ticos de vuelta a di√°logos
        print("\nüîÑ Reconstruyendo di√°logos sint√©ticos...")
        
        balanced_examples = []
        original_count = len(examples)
        
        # Agregar ejemplos originales
        balanced_examples.extend(examples)
        
        # Agregar ejemplos sint√©ticos
        for i in range(original_count, len(balanced_embeddings)):
            synthetic_embedding = balanced_embeddings[i]
            target_class = balanced_labels[i]
            
            # Convertir embedding a di√°logo
            synthetic_dialogue = self.embedding_to_dialogue(
                synthetic_embedding, 
                original_examples, 
                target_class
            )
            
            balanced_examples.append(synthetic_dialogue)
        
        # 7. Guardar dataset balanceado
        self.save_balanced_dataset(balanced_examples)
        
        # 8. Generar estad√≠sticas finales
        self.generate_final_statistics(balanced_examples, original_count)
        
        return balanced_examples
    
    def save_balanced_dataset(self, examples: List[Dict]):
        """Guardar dataset balanceado"""
        print(f"\nüíæ Guardando dataset balanceado: {len(examples)} ejemplos")
        
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for example in examples:
                f.write(json.dumps(example, ensure_ascii=False) + '\n')
        
        print(f"‚úÖ Dataset guardado en: {self.output_file}")
    
    def generate_final_statistics(self, examples: List[Dict], original_count: int):
        """Generar estad√≠sticas del dataset balanceado"""
        print("\nüìä ESTAD√çSTICAS FINALES DEL DATASET BALANCEADO:")
        print("=" * 60)
        
        total_examples = len(examples)
        synthetic_count = total_examples - original_count
        
        print(f"üìù Total de ejemplos: {total_examples}")
        print(f"üìÅ Ejemplos originales: {original_count}")
        print(f"üîß Ejemplos sint√©ticos (SMOTE): {synthetic_count}")
        print(f"üìà Incremento: {(synthetic_count/original_count)*100:.1f}%")
        
        # Distribuci√≥n final por clase
        final_dist = Counter([ex['labels']['risk_class'] for ex in examples])
        
        print("\nüìÇ Distribuci√≥n final por clase de riesgo:")
        for risk_class, count in sorted(final_dist.items()):
            percentage = (count / total_examples) * 100
            print(f"  {risk_class:20s}: {count:5d} ({percentage:5.1f}%)")
        
        # M√©todos de generaci√≥n
        generation_methods = Counter([
            ex['meta'].get('generation_method', 'original_rag') 
            for ex in examples
        ])
        
        print("\nüõ†Ô∏è M√©todos de generaci√≥n:")
        for method, count in generation_methods.items():
            percentage = (count / total_examples) * 100
            print(f"  {method:20s}: {count:5d} ({percentage:5.1f}%)")

def main():
    """Funci√≥n principal"""
    print("üéØ SMOTE Dataset Balancer para TFM de Empat√≠a")
    print("=" * 60)
    
    # Configuraci√≥n
    input_file = "/home/ubuntu/empathy_dataset_final.jsonl"
    output_file = "/home/ubuntu/empathy_dataset_smote_balanced.jsonl"
    
    # Crear balanceador
    balancer = SMOTEDatasetBalancer(input_file, output_file)
    
    # Aplicar SMOTE
    balanced_dataset = balancer.apply_smote_balancing()
    
    print("\nüéâ ¬°Balanceamiento SMOTE completado exitosamente!")
    print(f"üìÅ Dataset balanceado disponible en: {output_file}")
    print(f"üìä Total de ejemplos: {len(balanced_dataset)}")

if __name__ == "__main__":
    main()
